{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing All Required Library"
      ],
      "metadata": {
        "id": "_jzb53ZgDW5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, Add, GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import MultiHeadAttention\n",
        "from tensorflow.keras import mixed_precision\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "_z-MuS9yDVnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading Eye Disease Datasets"
      ],
      "metadata": {
        "id": "14R3m9tnDdST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install gdown if not already installed\n",
        "!pip install -q gdown\n",
        "\n",
        "# Download the zip file using file ID from Google Drive\n",
        "!gdown --id 1vHCvd9ZFkY9lfNwUnnMUbDQZjjpby8lv -O disease.zip\n",
        "\n",
        "# Extract the ZIP file\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_file = \"disease.zip\"  # Name of the downloaded file\n",
        "\n",
        "# Make sure output directory exists\n",
        "output_dir = \"/content/extracted_file\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Extract the contents\n",
        "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(output_dir)\n",
        "\n",
        "print(\"Extraction completed!\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-15T10:13:09.635276Z",
          "iopub.execute_input": "2025-04-15T10:13:09.635509Z",
          "iopub.status.idle": "2025-04-15T10:14:24.561672Z",
          "shell.execute_reply.started": "2025-04-15T10:13:09.635485Z",
          "shell.execute_reply": "2025-04-15T10:14:24.560606Z"
        },
        "id": "-YCadb28C7ht",
        "outputId": "bb72b653-7a59-4312-d88a-943f87f81f14"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1QwItNnETzlWHgZ89EbHFYg04ijkDszF6\nFrom (redirected): https://drive.google.com/uc?id=1QwItNnETzlWHgZ89EbHFYg04ijkDszF6&confirm=t&uuid=dd8cae99-b0c3-43cc-b657-13e52c96e3d9\nTo: /kaggle/working/disease.zip\n100%|██████████████████████████████████████| 3.85G/3.85G [00:44<00:00, 85.9MB/s]\nExtraction completed!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vision Transformer Architecture and training it"
      ],
      "metadata": {
        "id": "qRTCVsKjDk-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable mixed precision\n",
        "mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "\n",
        "# Patch & Position Embedding Layer\n",
        "class PatchEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, patch_size, projection_dim):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.projection_dim = projection_dim\n",
        "        self.projection = tf.keras.layers.Conv2D(filters=projection_dim,\n",
        "                                                 kernel_size=patch_size,\n",
        "                                                 strides=patch_size,\n",
        "                                                 padding='valid')\n",
        "        self.flatten = tf.keras.layers.Reshape((-1, projection_dim))\n",
        "\n",
        "    def call(self, images):\n",
        "        x = self.projection(images)\n",
        "        x = self.flatten(x)\n",
        "        return x\n",
        "\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super().__init__()\n",
        "        self.pos_embedding = self.add_weight(\n",
        "            name=\"pos_embed\",\n",
        "            shape=(1, num_patches + 1, projection_dim),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True\n",
        "        )\n",
        "        self.cls_token = self.add_weight(\n",
        "            name=\"cls_token\",\n",
        "            shape=(1, 1, projection_dim),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        cls_tokens = tf.broadcast_to(self.cls_token, [batch_size, 1, tf.shape(x)[-1]])\n",
        "        x = tf.concat([cls_tokens, x], axis=1)\n",
        "        return x + self.pos_embedding\n",
        "\n",
        "\n",
        "# Transformer Encoder Block\n",
        "def transformer_encoder(x, projection_dim, num_heads, mlp_dim, dropout_rate):\n",
        "    x1 = LayerNormalization(epsilon=1e-6)(x)\n",
        "    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim)(x1, x1)\n",
        "    x2 = Add()([x, attention_output])\n",
        "    x3 = LayerNormalization(epsilon=1e-6)(x2)\n",
        "    x3 = Dense(mlp_dim, activation='gelu')(x3)\n",
        "    x3 = Dropout(dropout_rate)(x3)\n",
        "    x3 = Dense(projection_dim)(x3)\n",
        "    x3 = Dropout(dropout_rate)(x3)\n",
        "    return Add()([x2, x3])\n",
        "\n",
        "# Build ViT\n",
        "def build_vit(input_shape=(224, 224, 3), num_classes=7,\n",
        "              patch_size=16, projection_dim=256, transformer_layers=12,\n",
        "              num_heads=8, mlp_dim=512, dropout_rate=0.1):\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Patch Embedding + Positional Encoding\n",
        "    patch_embed = PatchEmbedding(patch_size, projection_dim)(inputs)\n",
        "    num_patches = (input_shape[0] // patch_size) * (input_shape[1] // patch_size)\n",
        "    encoded = PositionalEncoding(num_patches, projection_dim)(patch_embed)\n",
        "\n",
        "    # Transformer Encoder\n",
        "    for _ in range(transformer_layers):\n",
        "        encoded = transformer_encoder(encoded, projection_dim, num_heads, mlp_dim, dropout_rate)\n",
        "\n",
        "    # Classification head\n",
        "    x = LayerNormalization(epsilon=1e-6)(encoded)\n",
        "    x = x[:, 0, :]  # CLS token\n",
        "    x = Dropout(0.3)(x)\n",
        "    outputs = Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
        "\n",
        "    return Model(inputs, outputs)\n",
        "\n",
        "# ----------------------------\n",
        "# Training Pipeline\n",
        "# ----------------------------\n",
        "\n",
        "# Image size, batch size\n",
        "img_size = (224, 224)\n",
        "batch_size = 32\n",
        "data_path = \"/content/extracted_file/Retinal Fundus Images - Copy/train\"\n",
        "\n",
        "# Datasets\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_path,\n",
        "    validation_split=0.1,\n",
        "    subset='training',\n",
        "    seed=42,\n",
        "    image_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    label_mode='categorical'\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_path,\n",
        "    validation_split=0.1,\n",
        "    subset='validation',\n",
        "    seed=42,\n",
        "    image_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    label_mode='categorical'\n",
        ")\n",
        "\n",
        "# Augmentation + Normalization\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.Rescaling(1./255),\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.1),\n",
        "    tf.keras.layers.RandomZoom(0.1),\n",
        "])\n",
        "\n",
        "train_ds = train_ds.map(lambda x, y: (data_augmentation(x), y)).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.map(lambda x, y: (tf.keras.layers.Rescaling(1./255)(x), y)).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Build and compile\n",
        "vit_model = build_vit()\n",
        "vit_model.compile(optimizer=Adam(1e-4),\n",
        "                  loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Train\n",
        "callbacks = [\n",
        "    ModelCheckpoint(\"/content/best_vit_model.keras\", save_best_only=True),\n",
        "    # EarlyStopping(patience=5, restore_best_weights=True)\n",
        "]\n",
        "\n",
        "vit_model.fit(train_ds, validation_data=val_ds, epochs=30, callbacks=callbacks)\n",
        "\n",
        "# Evaluate\n",
        "loss, acc = vit_model.evaluate(val_ds)\n",
        "print(f\"Final Accuracy: {acc * 100:.2f}%\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-15T10:14:30.237919Z",
          "iopub.execute_input": "2025-04-15T10:14:30.238400Z",
          "iopub.status.idle": "2025-04-15T16:13:51.480902Z",
          "shell.execute_reply.started": "2025-04-15T10:14:30.238375Z",
          "shell.execute_reply": "2025-04-15T16:13:51.480298Z"
        },
        "id": "yihveXz8C7hv",
        "outputId": "25740f3a-b4e4-4f3d-a3ac-1fb14b70e939"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-04-15 10:14:32.553202: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744712072.749709      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744712072.805162      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Found 55446 files belonging to 7 classes.\nUsing 49902 files for training.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "I0000 00:00:1744712088.330170      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Found 55446 files belonging to 7 classes.\nUsing 5544 files for validation.\nEpoch 1/30\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1744712148.670470     105 service.cc:148] XLA service 0x7bbfb4004ca0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1744712148.671365     105 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1744712154.505427     105 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1744712181.939858     105 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m839s\u001b[0m 481ms/step - accuracy: 0.2794 - loss: 1.9586 - val_accuracy: 0.5422 - val_loss: 1.3059\nEpoch 2/30\n\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m718s\u001b[0m 460ms/step - accuracy: 0.5476 - loss: 1.3175 - val_accuracy: 0.6533 - val_loss: 1.1513\nEpoch 3/30\n\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m716s\u001b[0m 458ms/step - accuracy: 0.6138 - loss: 1.2022 - val_accuracy: 0.6595 - val_loss: 1.1105\nEpoch 4/30\n\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m713s\u001b[0m 457ms/step - accuracy: 0.6427 - loss: 1.1440 - val_accuracy: 0.6732 - val_loss: 1.0852\nEpoch 5/30\n\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m714s\u001b[0m 457ms/step - accuracy: 0.6701 - loss: 1.0942 - val_accuracy: 0.7098 - val_loss: 1.0121\nEpoch 6/30\n\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m712s\u001b[0m 456ms/step - accuracy: 0.6883 - loss: 1.0557 - val_accuracy: 0.7087 - val_loss: 1.0049\nEpoch 7/30\n\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m701s\u001b[0m 449ms/step - accuracy: 0.7115 - loss: 1.0127 - val_accuracy: 0.7287 - val_loss: 0.9754\nEpoch 8/30\n\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m721s\u001b[0m 462ms/step - accuracy: 0.7344 - loss: 0.9758 - val_accuracy: 0.7558 - val_loss: 0.9262\nEpoch 9/30\n\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m719s\u001b[0m 460ms/step - accuracy: 0.7520 - loss: 0.9420 - val_accuracy: 0.7783 - val_loss: 0.8813\nEpoch 10/30\n\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m715s\u001b[0m 458ms/step - accuracy: 0.7671 - loss: 0.9069 - val_accuracy: 0.7835 - val_loss: 0.8684\nEpoch 11/30\n\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m726s\u001b[0m 465ms/step - accuracy: 0.7870 - loss: 0.8718 - val_accuracy: 0.8057 - val_loss: 0.8270\nEpoch 12/30\n\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m704s\u001b[0m 451ms/step - accuracy: 0.7968 - loss: 0.8499 - val_accuracy: 0.7884 - val_loss: 0.8538\nEpoch 13/30\n\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m728s\u001b[0m 466ms/step - accuracy: 0.8139 - loss: 0.8186 - val_accuracy: 0.8357 - val_loss: 0.7856\nEpoch 14/30\n\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m709s\u001b[0m 454ms/step - accuracy: 0.8275 - loss: 0.7931 - val_accuracy: 0.8391 - val_loss: 0.7626\nEpoch 15/30\n\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m729s\u001b[0m 467ms/step - accuracy: 0.8403 - loss: 0.7641 - val_accuracy: 0.8557 - val_loss: 0.7412\nEpoch 16/30\n\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m726s\u001b[0m 465ms/step - accuracy: 0.8557 - loss: 0.7363 - val_accuracy: 0.8772 - val_loss: 0.6929\nEpoch 17/30\n\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m704s\u001b[0m 451ms/step - accuracy: 0.8699 - loss: 0.7133 - val_accuracy: 0.8754 - val_loss: 0.6942\nEpoch 18/30\n\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m697s\u001b[0m 447ms/step - accuracy: 0.8784 - loss: 0.6938 - val_accuracy: 0.8727 - val_loss: 0.6998\nEpoch 19/30\n\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m720s\u001b[0m 461ms/step - accuracy: 0.8850 - loss: 0.6793 - val_accuracy: 0.8977 - val_loss: 0.6486\nEpoch 20/30\n\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m708s\u001b[0m 454ms/step - accuracy: 0.8978 - loss: 0.6569 - val_accuracy: 0.9069 - val_loss: 0.6350\nEpoch 21/30\n\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m703s\u001b[0m 450ms/step - accuracy: 0.9008 - loss: 0.6482 - val_accuracy: 0.9046 - val_loss: 0.6363\nEpoch 22/30\n\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m700s\u001b[0m 449ms/step - accuracy: 0.9063 - loss: 0.6380 - val_accuracy: 0.9085 - val_loss: 0.6203\nEpoch 23/30\n\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m706s\u001b[0m 452ms/step - accuracy: 0.9120 - loss: 0.6263 - val_accuracy: 0.9226 - val_loss: 0.6028\nEpoch 24/30\n\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m720s\u001b[0m 462ms/step - accuracy: 0.9175 - loss: 0.6146 - val_accuracy: 0.9147 - val_loss: 0.6121\nEpoch 25/30\n\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m711s\u001b[0m 455ms/step - accuracy: 0.9209 - loss: 0.6067 - val_accuracy: 0.9288 - val_loss: 0.5787\nEpoch 26/30\n\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m718s\u001b[0m 460ms/step - accuracy: 0.9277 - loss: 0.5936 - val_accuracy: 0.9251 - val_loss: 0.5886\nEpoch 27/30\n\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m702s\u001b[0m 450ms/step - accuracy: 0.9317 - loss: 0.5879 - val_accuracy: 0.9304 - val_loss: 0.5858\nEpoch 28/30\n\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m716s\u001b[0m 459ms/step - accuracy: 0.9314 - loss: 0.5846 - val_accuracy: 0.9275 - val_loss: 0.5837\nEpoch 29/30\n\u001b[1m1560/1560\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m707s\u001b[0m 453ms/step - accuracy: 0.9387 - loss: 0.5688 - val_accuracy: 0.9370 - val_loss: 0.5685\n\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 96ms/step - accuracy: 0.9379 - loss: 0.5646\nFinal Accuracy: 93.70%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the Vision Transformer Model"
      ],
      "metadata": {
        "id": "MfnmBzINDtU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save entire model\n",
        "vit_model.save('/content/vit_model.keras')  # or .keras"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-15T16:17:15.435185Z",
          "iopub.execute_input": "2025-04-15T16:17:15.435480Z",
          "iopub.status.idle": "2025-04-15T16:17:17.877052Z",
          "shell.execute_reply.started": "2025-04-15T16:17:15.435457Z",
          "shell.execute_reply": "2025-04-15T16:17:17.876144Z"
        },
        "id": "NCt1ZThYC7hy"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}